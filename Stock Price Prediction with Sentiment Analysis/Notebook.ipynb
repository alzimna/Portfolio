{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMAENoXvW8NNccKUapbo4nf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alzimna/Portfolio/blob/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Overview**"
      ],
      "metadata": {
        "id": "k3mq6lHmlLWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada akhir tahun 2023, hubungan antara Palestina dan Israel memanas akibat penyerangan Israel di Gaza, Palestina. Penyerangan ini tidak hanya berdampak pada rakyat Palestina saja namun juga memberikan bekas luka kepada rakyat muslim di seluruh dunia. Hal ini memicu perlawanan dari seluruh negara muslim termasuk Indonesia sendiri.\n",
        "\n",
        "Di Indonesia, santer terdengar pernyataan boikot terhadap produk yang diduga menyalurkan dana kepada Israel. Setelah kami lakukan investigasi di Twitter, perusahaan seperti Unilever dan KHC menjadi topik perbincangan teratas.\n",
        "\n",
        "Projek ini merupakan projek yang dilakukan oleh saya dan 3 orang teman saya. Tujuan dari produk ini adalah menginvestigasi efek dari boikot melalui analisis sentimen di Twitter pada kenaikan harga sahan dari Unilever (UNVR) lau membandingkannya dengan perusahaan yang terkena boikot namun tidak menjadi trending topic di twitter seperti KHC."
      ],
      "metadata": {
        "id": "9wYl7Vwbge1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Method**\n",
        "Berikut ini langkah-langkah yang dilakukan pada projek kali ini.\n",
        "\n",
        "1. Scrapping tweet dengan kata kunci \"UNVR\", \"Unilever\", \"boikot unilever\".\n",
        "2. Data preprocessing seperti\n",
        "  * Menghapus duplikat\n",
        "  * Mengubah format tanggal\n",
        "  * Menghilangkan simbol khusus seperti \\r, \\n, \", url, dan byte\n",
        "  * Mengubah menjadi lower case\n",
        "  * Menghapus simbol dan tanda baca\n",
        "  * Menormalisasi kata\n",
        "  * Menghapus kata tidak bermakna\n",
        "  * Menghapus imbuhan\n",
        "  * Tokenize data\n",
        "3. Analisis data eksploratif\n",
        "4. Mengambil data saham UNVR dan KHC dari yahoo finance\n",
        "5. Melakukan analisis sentimen pada tiap tweet\n",
        "6. Mengklasifikasikan hari berdasarkan kejadian Naik/Turunnya harga saham UNVR\n",
        "7. Melakukan klasifikasi pada data yang dilengkapi polarity dan tidak untuk dibandingkan akurasinya. Model yang dipakai antara lain Regresi Logistik, Linear Discriminant Analysis (LDA), KNN, dan SVM.\n",
        "\n",
        "Kesimpulan yang dapat diperoleh dari projek ini adalah **Polarity akan menaikkan akurasi model jika dibandingkan dengan data tanpa polaritas. Artinya analisis sentimen dapat membantu untuk menaikkan akurasi model biasa untuk mengklasifikasian Naik/Turunnya harga saham**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yvLFzUqmsDXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code Snippet**"
      ],
      "metadata": {
        "id": "0SH0yg5osQse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krOnDWrn26KC",
        "outputId": "1577b4ee-4614-4442-e900-42073647a892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/site-packages (1.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.11/site-packages (4.0.2)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/site-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/site-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/site-packages (from httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/site-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.14.0)\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "\n",
        "# For EDA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import missingno as msno\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "import nltk\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "#For Pre-Processing\n",
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import os\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "import string\n",
        "\n",
        "from textblob import TextBlob\n",
        "from IPython.display import clear_output\n",
        "!pip install googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data berikut ini merupakan hasil scrapping tweet dengan kata kunci \"UNVR\", \"Unilever\", dan \"boikot unilever\". Ketiga data tersebut disatukan untuk dilakukan preprocessing."
      ],
      "metadata": {
        "id": "EUX83evnt-O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = {}\n",
        "url[1] = \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/Data_UNVR.xlsx\"\n",
        "url[2] = 'https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/Unilever.xlsx'\n",
        "url[3] = 'https://github.com/alzimna/Portfolio/blob/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/boikot_unilever.xlsx'\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for i in range(1,3) :\n",
        "  temp = pd.read_excel(url[i])\n",
        "  df = pd.concat([df,temp],axis = 0)\n",
        "\n",
        "data = df[[\"created_at\",\"username\",\"tweet_url\",\"full_text\"]]\n",
        "data = data.rename(columns = {\n",
        "    \"created_at\" : \"Tanggal\",\n",
        "    \"tweet_url\" : \"Link\",\n",
        "    \"full_text\" : \"Konten\"\n",
        "})\n",
        "\n",
        "data.reset_index(inplace = True,drop = True)\n",
        "data.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "lBNrbms53gza",
        "outputId": "73d1748f-2005-44ac-cb2b-42078d67b171"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           Tanggal         username  \\\n",
              "0   Wed Dec 06 22:34:21 +0000 2023     ataritafaran   \n",
              "1   Wed Dec 06 22:04:00 +0000 2023     nooptionmood   \n",
              "2   Wed Dec 06 16:07:27 +0000 2023        rnamidubx   \n",
              "3   Wed Dec 06 00:35:52 +0000 2023     doktermarket   \n",
              "4   Tue Dec 05 23:23:47 +0000 2023    a37723_fauzan   \n",
              "5   Tue Dec 05 05:41:08 +0000 2023        Bisniscom   \n",
              "6   Tue Dec 05 05:37:10 +0000 2023     KATADATAcoid   \n",
              "7   Mon Dec 04 23:50:11 +0000 2023       InvestorID   \n",
              "8   Mon Dec 04 23:41:04 +0000 2023        Bisniscom   \n",
              "9   Mon Dec 04 09:06:49 +0000 2023       Iilasikuta   \n",
              "10  Mon Dec 04 06:27:00 +0000 2023     LumbungSaham   \n",
              "11  Mon Dec 04 03:52:28 +0000 2023        Bisniscom   \n",
              "12  Mon Dec 04 03:52:28 +0000 2023        Bisniscom   \n",
              "13  Mon Dec 04 01:08:07 +0000 2023       InvestorID   \n",
              "14  Mon Dec 04 01:03:34 +0000 2023        Bisniscom   \n",
              "15  Mon Dec 04 01:03:07 +0000 2023  rahmat_wirmrtan   \n",
              "16  Mon Dec 04 00:29:07 +0000 2023     doktermarket   \n",
              "17  Sun Dec 03 23:23:13 +0000 2023        Bisniscom   \n",
              "18  Sun Dec 03 15:12:15 +0000 2023    xxxsyntaxeror   \n",
              "19  Sun Dec 03 11:27:33 +0000 2023        Bisniscom   \n",
              "\n",
              "                                                 Link  \\\n",
              "0   https://twitter.com/ataritafaran/status/173252...   \n",
              "1   https://twitter.com/nooptionmood/status/173252...   \n",
              "2   https://twitter.com/rnamidubx/status/173243157...   \n",
              "3   https://twitter.com/doktermarket/status/173219...   \n",
              "4   https://twitter.com/a37723_fauzan/status/17321...   \n",
              "5   https://twitter.com/Bisniscom/status/173191156...   \n",
              "6   https://twitter.com/KATADATAcoid/status/173191...   \n",
              "7   https://twitter.com/InvestorID/status/17318232...   \n",
              "8   https://twitter.com/Bisniscom/status/173182094...   \n",
              "9   https://twitter.com/Iilasikuta/status/17316009...   \n",
              "10  https://twitter.com/LumbungSaham/status/173156...   \n",
              "11  https://twitter.com/Bisniscom/status/173152182...   \n",
              "12  https://twitter.com/Bisniscom/status/173152182...   \n",
              "13  https://twitter.com/InvestorID/status/17314804...   \n",
              "14  https://twitter.com/Bisniscom/status/173147932...   \n",
              "15  https://twitter.com/rahmat_wirmrtan/status/173...   \n",
              "16  https://twitter.com/doktermarket/status/173147...   \n",
              "17  https://twitter.com/Bisniscom/status/173145406...   \n",
              "18  https://twitter.com/xxxsyntaxeror/status/17313...   \n",
              "19  https://twitter.com/Bisniscom/status/173127396...   \n",
              "\n",
              "                                               Konten  \n",
              "0   @rasyidrobbani @anang_kur @rykarlsen Kalau ord...  \n",
              "1   @rykarlsen Sepupu Di unvr infonya masih stabil...  \n",
              "2   @rykarlsen Pt kantor sy ga kena boikot dan ga ...  \n",
              "3   Pagi. semalam bursa AS ditutup bervariasi. Dow...  \n",
              "4   melihat adanya kenaikan yang stabil dalam bebe...  \n",
              "5   4 Direksi Unilever Indonesia (UNVR) Mundur, Be...  \n",
              "6   IHSG Sesi I Turun 0,4%, Saham UNVR hingga GOTO...  \n",
              "7   MNC Sekuritas memprediksi IHSG hari ini terkor...  \n",
              "8   IHSG Hari Ini Menuju 7.130, Cermati Saham BBCA...  \n",
              "9           @parxhanbin @Nakyunga IYAAA!! KENAPA UNVR  \n",
              "10  Hari ini cum dividen di Pasar Reguler!  PT Uni...  \n",
              "11  Ira menjual 870.000 saham UNVR atau setara 0,0...  \n",
              "12  Sebelum Resign, Ira Noviarti Jual Saham Unilev...  \n",
              "13  Unilever Indonesia (UNVR) akan membagikan divi...  \n",
              "14  Sebelum Resign, Ira Noviarti Jual Saham Unilev...  \n",
              "15  @_adlsr @SteveeeKlax @kuponoia @worksfess Emit...  \n",
              "16  Pagi. Jumat lalu indeks bursa AS melanjutkan p...  \n",
              "17  Jadwal Dividen 11 Emiten Pekan Ini, dari UNVR,...  \n",
              "18  @weunderstandyo @worksfess bangkeee ini bocah,...  \n",
              "19  Siap-siap Cum Dividen Jumbo Unilever (UNVR), C...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd2b37d2-8ff6-47c9-bd66-f3eb4a437160\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tanggal</th>\n",
              "      <th>username</th>\n",
              "      <th>Link</th>\n",
              "      <th>Konten</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wed Dec 06 22:34:21 +0000 2023</td>\n",
              "      <td>ataritafaran</td>\n",
              "      <td>https://twitter.com/ataritafaran/status/173252...</td>\n",
              "      <td>@rasyidrobbani @anang_kur @rykarlsen Kalau ord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wed Dec 06 22:04:00 +0000 2023</td>\n",
              "      <td>nooptionmood</td>\n",
              "      <td>https://twitter.com/nooptionmood/status/173252...</td>\n",
              "      <td>@rykarlsen Sepupu Di unvr infonya masih stabil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wed Dec 06 16:07:27 +0000 2023</td>\n",
              "      <td>rnamidubx</td>\n",
              "      <td>https://twitter.com/rnamidubx/status/173243157...</td>\n",
              "      <td>@rykarlsen Pt kantor sy ga kena boikot dan ga ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wed Dec 06 00:35:52 +0000 2023</td>\n",
              "      <td>doktermarket</td>\n",
              "      <td>https://twitter.com/doktermarket/status/173219...</td>\n",
              "      <td>Pagi. semalam bursa AS ditutup bervariasi. Dow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tue Dec 05 23:23:47 +0000 2023</td>\n",
              "      <td>a37723_fauzan</td>\n",
              "      <td>https://twitter.com/a37723_fauzan/status/17321...</td>\n",
              "      <td>melihat adanya kenaikan yang stabil dalam bebe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Tue Dec 05 05:41:08 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173191156...</td>\n",
              "      <td>4 Direksi Unilever Indonesia (UNVR) Mundur, Be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Tue Dec 05 05:37:10 +0000 2023</td>\n",
              "      <td>KATADATAcoid</td>\n",
              "      <td>https://twitter.com/KATADATAcoid/status/173191...</td>\n",
              "      <td>IHSG Sesi I Turun 0,4%, Saham UNVR hingga GOTO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mon Dec 04 23:50:11 +0000 2023</td>\n",
              "      <td>InvestorID</td>\n",
              "      <td>https://twitter.com/InvestorID/status/17318232...</td>\n",
              "      <td>MNC Sekuritas memprediksi IHSG hari ini terkor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Mon Dec 04 23:41:04 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173182094...</td>\n",
              "      <td>IHSG Hari Ini Menuju 7.130, Cermati Saham BBCA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Mon Dec 04 09:06:49 +0000 2023</td>\n",
              "      <td>Iilasikuta</td>\n",
              "      <td>https://twitter.com/Iilasikuta/status/17316009...</td>\n",
              "      <td>@parxhanbin @Nakyunga IYAAA!! KENAPA UNVR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mon Dec 04 06:27:00 +0000 2023</td>\n",
              "      <td>LumbungSaham</td>\n",
              "      <td>https://twitter.com/LumbungSaham/status/173156...</td>\n",
              "      <td>Hari ini cum dividen di Pasar Reguler!  PT Uni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Mon Dec 04 03:52:28 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173152182...</td>\n",
              "      <td>Ira menjual 870.000 saham UNVR atau setara 0,0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Mon Dec 04 03:52:28 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173152182...</td>\n",
              "      <td>Sebelum Resign, Ira Noviarti Jual Saham Unilev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Mon Dec 04 01:08:07 +0000 2023</td>\n",
              "      <td>InvestorID</td>\n",
              "      <td>https://twitter.com/InvestorID/status/17314804...</td>\n",
              "      <td>Unilever Indonesia (UNVR) akan membagikan divi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Mon Dec 04 01:03:34 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173147932...</td>\n",
              "      <td>Sebelum Resign, Ira Noviarti Jual Saham Unilev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Mon Dec 04 01:03:07 +0000 2023</td>\n",
              "      <td>rahmat_wirmrtan</td>\n",
              "      <td>https://twitter.com/rahmat_wirmrtan/status/173...</td>\n",
              "      <td>@_adlsr @SteveeeKlax @kuponoia @worksfess Emit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Mon Dec 04 00:29:07 +0000 2023</td>\n",
              "      <td>doktermarket</td>\n",
              "      <td>https://twitter.com/doktermarket/status/173147...</td>\n",
              "      <td>Pagi. Jumat lalu indeks bursa AS melanjutkan p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Sun Dec 03 23:23:13 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173145406...</td>\n",
              "      <td>Jadwal Dividen 11 Emiten Pekan Ini, dari UNVR,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Sun Dec 03 15:12:15 +0000 2023</td>\n",
              "      <td>xxxsyntaxeror</td>\n",
              "      <td>https://twitter.com/xxxsyntaxeror/status/17313...</td>\n",
              "      <td>@weunderstandyo @worksfess bangkeee ini bocah,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Sun Dec 03 11:27:33 +0000 2023</td>\n",
              "      <td>Bisniscom</td>\n",
              "      <td>https://twitter.com/Bisniscom/status/173127396...</td>\n",
              "      <td>Siap-siap Cum Dividen Jumbo Unilever (UNVR), C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd2b37d2-8ff6-47c9-bd66-f3eb4a437160')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cd2b37d2-8ff6-47c9-bd66-f3eb4a437160 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cd2b37d2-8ff6-47c9-bd66-f3eb4a437160');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ea2b6f53-e127-4e9e-80b1-cba399433d88\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea2b6f53-e127-4e9e-80b1-cba399433d88')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ea2b6f53-e127-4e9e-80b1-cba399433d88 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 6294,\n  \"fields\": [\n    {\n      \"column\": \"Tanggal\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 6197,\n        \"samples\": [\n          \"Tue Oct 31 10:13:24 +0000 2023\",\n          \"Wed Nov 01 04:04:45 +0000 2023\",\n          \"Sun Nov 26 12:12:29 +0000 2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"username\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4437,\n        \"samples\": [\n          \"highbreakdown\",\n          \"istjdream\",\n          \"oXznaynuy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6215,\n        \"samples\": [\n          \"https://twitter.com/merindaaaa/status/1720281713535054060\",\n          \"https://twitter.com/pituturid_/status/1725411888006119660\",\n          \"https://twitter.com/s_4444x/status/1718980354089091539\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Konten\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6205,\n        \"samples\": [\n          \"peningkatan yang luar biasa.   Selamat kepada seluruh tim PT BGR Logistik Cabang Padang dan Cabang Palembang atas dedikasi dan kerja keras. Keberhasilan ini adalah bukti nyata dari komitmen Unilever Indonesia dalam menciptakan kemitraan yang kuat dan berkelanjutan.\",\n          \"Tiga Petinggi Unilever Kompak Mengundurkan Diri https://t.co/25SyOBRFDv\",\n          \"@Versaachae Wkwkwkwk kebetulan berarti  Soalnya diskon itu dari Unilever dan ultra bukan produk Unilever setauku\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "s_Z8p1a9uydE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menghapus duplikat"
      ],
      "metadata": {
        "id": "Ev-TY3u2u4rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengecekan data duplikasi\n",
        "print(\"data duplikat ada :\",data.duplicated().sum())\n",
        "\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Pengecekan data duplikasi kedya\n",
        "print(\"data duplikat ada :\", data.duplicated().sum())\n",
        "\n",
        "#Menghapus baris yang memuat na\n",
        "data = data.dropna()\n",
        "\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqKKLrQW8CAh",
        "outputId": "d1338e8f-e6b2-4cf4-fc6f-a609cb80b14f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data duplikat ada : 79\n",
            "data duplikat ada : 0\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6213 entries, 0 to 6293\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Tanggal   6213 non-null   object\n",
            " 1   username  6213 non-null   object\n",
            " 2   Link      6213 non-null   object\n",
            " 3   Konten    6213 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 242.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mengubah format tanggal"
      ],
      "metadata": {
        "id": "dL-Ke8bnu89n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "temp = []\n",
        "for i in range(data.shape[0]) :\n",
        "  cek = data.iloc[i][\"Tanggal\"]\n",
        "  s = cek[8:10]+\" \"+cek[4:7]+\" \"+cek[-4:]\n",
        "  s = datetime.strptime(s, \"%d %b %Y\")\n",
        "  temp.append(s)\n",
        "data[\"Tanggal\"] = temp\n",
        "\n",
        "data = data.sort_values(by='Tanggal')\n",
        "data.reset_index(inplace = True,drop = True)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "K21Bs_0h9Ml6",
        "outputId": "72df525c-838e-4db5-c796-1242a1cbecfe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Tanggal         username  \\\n",
              "0 2023-10-07        ijcancer_   \n",
              "1 2023-10-07     MakassarFess   \n",
              "2 2023-10-07  PurazaAnalytics   \n",
              "3 2023-10-07     selembutbolu   \n",
              "4 2023-10-07        ncaaidnzz   \n",
              "\n",
              "                                                Link  \\\n",
              "0  https://twitter.com/ijcancer_/status/171048510...   \n",
              "1  https://twitter.com/MakassarFess/status/171048...   \n",
              "2  https://twitter.com/PurazaAnalytics/status/171...   \n",
              "3  https://twitter.com/selembutbolu/status/171070...   \n",
              "4  https://twitter.com/ncaaidnzz/status/171069358...   \n",
              "\n",
              "                                              Konten  \n",
              "0  Mantan presbem, kerja di unilever, keterima di...  \n",
              "1  Tabe info info yg bingung mau kemana weekendny...  \n",
              "2  Sebesar besar company macam Unilever pun guna ...  \n",
              "3     @pengwinningg Mangat unilever ðŸ¤­ðŸ’‹ðŸ’‹ðŸ’‹  \n",
              "4  @UPIfess unilever AAMIINN PLEASEEEE selain dek...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23e8b818-3616-481b-8382-ac471666322d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tanggal</th>\n",
              "      <th>username</th>\n",
              "      <th>Link</th>\n",
              "      <th>Konten</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-10-07</td>\n",
              "      <td>ijcancer_</td>\n",
              "      <td>https://twitter.com/ijcancer_/status/171048510...</td>\n",
              "      <td>Mantan presbem, kerja di unilever, keterima di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-10-07</td>\n",
              "      <td>MakassarFess</td>\n",
              "      <td>https://twitter.com/MakassarFess/status/171048...</td>\n",
              "      <td>Tabe info info yg bingung mau kemana weekendny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-10-07</td>\n",
              "      <td>PurazaAnalytics</td>\n",
              "      <td>https://twitter.com/PurazaAnalytics/status/171...</td>\n",
              "      <td>Sebesar besar company macam Unilever pun guna ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-10-07</td>\n",
              "      <td>selembutbolu</td>\n",
              "      <td>https://twitter.com/selembutbolu/status/171070...</td>\n",
              "      <td>@pengwinningg Mangat unilever ðŸ¤­ðŸ’‹ðŸ’‹ðŸ’‹</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-10-07</td>\n",
              "      <td>ncaaidnzz</td>\n",
              "      <td>https://twitter.com/ncaaidnzz/status/171069358...</td>\n",
              "      <td>@UPIfess unilever AAMIINN PLEASEEEE selain dek...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23e8b818-3616-481b-8382-ac471666322d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23e8b818-3616-481b-8382-ac471666322d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23e8b818-3616-481b-8382-ac471666322d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-edd7ad5a-038b-43e3-8225-4b4075b0c2e9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-edd7ad5a-038b-43e3-8225-4b4075b0c2e9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-edd7ad5a-038b-43e3-8225-4b4075b0c2e9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 6213,\n  \"fields\": [\n    {\n      \"column\": \"Tanggal\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-10-07 00:00:00\",\n        \"max\": \"2023-12-07 00:00:00\",\n        \"num_unique_values\": 62,\n        \"samples\": [\n          \"2023-11-26 00:00:00\",\n          \"2023-12-02 00:00:00\",\n          \"2023-10-07 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"username\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4435,\n        \"samples\": [\n          \"TenTen2658\",\n          \"nanazara_\",\n          \"estehmaduu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6213,\n        \"samples\": [\n          \"https://twitter.com/Pramuwardhanie/status/1726376689645875204\",\n          \"https://twitter.com/catforlyfee/status/1720768698255126917\",\n          \"https://twitter.com/tempodotco/status/1728215193191383281\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Konten\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6205,\n        \"samples\": [\n          \"@sagaraalmer @Unilever @Danone Alhamdulillah semenjak saya lihat unilever dukung LGBT di IG beberapa tahun lalu, dirumah sdh tdk ada produk dari unilever. Termasuk air minum pun sudah tdk beli Aqua lagi.  Anak2 pun sudah paham mana produk yg harus dijauhi\",\n          \"Sehat terus pak!\\u00f0\\u0178\\u2122\\u0152 biar bisa handle ini Negara  Prabowo Subianto Kiky Rocket Chicken Indonesia Jatinangor Gal Gadot Kalimantan Halloween #GISELLE Unilever Apel Pagi Senin #FreePalestine Olive\",\n          \"@republikaonline sa punya toko sembako, semua produk2 unilever sa berenti jual.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menghilangkan simbol khusus seperti \\r, \\n, \", url, dan byte"
      ],
      "metadata": {
        "id": "uPGr-vUQvf0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT PRE-PROCESSING\n",
        "# TAHAP 1\n",
        "# Menghilangkan simbol khusus seperti \\r, \\n, \", url, dan byte\n",
        "def cleaning(data):\n",
        "  data['Konten_Parsed_1'] = data['Konten'].str.replace(\"\\\\\\\\r\",\n",
        "  \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"\\\\\\\\n\", \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"\\r\", \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"&amp\", \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"&gt\", \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"&lt\", \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\" \", \" \")\n",
        "  data['Konten_Parsed_1']= data['Konten_Parsed_1'].str.replace(r'''(?i)\\b((?:https|http?:/\n",
        "  /|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][az]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\n",
        "  \\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”\n",
        "  ‘’]))''', \" \")\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace('\"', '')\n",
        "  data['Konten_Parsed_1'] = data['Konten_Parsed_1'].str.replace(\"\\\\\\\\x[a-zA-z0-9][a-zA-z0-9]\",\"\")\n",
        "  return data\n",
        "\n",
        "data = cleaning(data)\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data[\"Konten\"][0])\n",
        "print(\"Hasil setelah tahap 1 :\\n\",data[\"Konten_Parsed_1\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWxLynwMGeOy",
        "outputId": "66e905d3-6625-43ad-8d21-e4bf90650f80"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampel data sebelum preprocess :\n",
            " Mantan presbem, kerja di unilever, keterima di pbb gadiambil karena pengennya di unicef,  mau nyoba ke Freeport. Disukai temen kantor, cewek UI, pinter, volunteer luar negeri, cantik lagi. Sempurna banget gak tu idup, lupa nanya linkedinnya ðŸ¥², jaringan keputus\n",
            "Hasil setelah tahap 1 :\n",
            " Mantan presbem, kerja di unilever, keterima di pbb gadiambil karena pengennya di unicef,  mau nyoba ke Freeport. Disukai temen kantor, cewek UI, pinter, volunteer luar negeri, cantik lagi. Sempurna banget gak tu idup, lupa nanya linkedinnya ðŸ¥², jaringan keputus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mengubah menjadi lower case"
      ],
      "metadata": {
        "id": "7lOAUZ8Rv7MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TAHAP 2\n",
        "# Mengubah menjadi lower case\n",
        "def lowerCase(data):\n",
        "    data['Konten_Parsed_2'] = data['Konten_Parsed_1'].str.lower()\n",
        "    return data\n",
        "data = lowerCase(data)\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data[\"Konten\"][0])\n",
        "print(\"Hasil setelah tahap 2 :\\n\",data[\"Konten_Parsed_2\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0P9QXAtPW4p",
        "outputId": "c4d4f298-48fb-4948-893f-d8899dd58b78"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampel data sebelum preprocess :\n",
            " Mantan presbem, kerja di unilever, keterima di pbb gadiambil karena pengennya di unicef,  mau nyoba ke Freeport. Disukai temen kantor, cewek UI, pinter, volunteer luar negeri, cantik lagi. Sempurna banget gak tu idup, lupa nanya linkedinnya ðŸ¥², jaringan keputus\n",
            "Hasil setelah tahap 2 :\n",
            " mantan presbem, kerja di unilever, keterima di pbb gadiambil karena pengennya di unicef,  mau nyoba ke freeport. disukai temen kantor, cewek ui, pinter, volunteer luar negeri, cantik lagi. sempurna banget gak tu idup, lupa nanya linkedinnya ðÿ¥², jaringan keputus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menghapus simbol dan tanda baca"
      ],
      "metadata": {
        "id": "XzbvYYoIwDoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TAHAP 3\n",
        "#Menghapus simbol dan tanda baca\n",
        "def removePunct(data):\n",
        "    data['Konten_Parsed_3'] = data['Konten_Parsed_2']\n",
        "    data['Konten_Parsed_3'] = data['Konten_Parsed_3'].str.replace(\"'s\",\"\")\n",
        "    data['Konten_Parsed_3'] = data['Konten_Parsed_3'].str.replace('[^a-zA-Z0-9]+',' ')\n",
        "    return data\n",
        "data = removePunct(data)\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data[\"Konten\"][0])\n",
        "print(\"Hasil setelah tahap 3 :\\n\",data[\"Konten_Parsed_3\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RGLvsp1QMrK",
        "outputId": "8dbd8013-001b-4441-8daf-d9a87ce66078"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampel data sebelum preprocess :\n",
            " Mantan presbem, kerja di unilever, keterima di pbb gadiambil karena pengennya di unicef,  mau nyoba ke Freeport. Disukai temen kantor, cewek UI, pinter, volunteer luar negeri, cantik lagi. Sempurna banget gak tu idup, lupa nanya linkedinnya ðŸ¥², jaringan keputus\n",
            "Hasil setelah tahap 3 :\n",
            " mantan presbem, kerja di unilever, keterima di pbb gadiambil karena pengennya di unicef,  mau nyoba ke freeport. disukai temen kantor, cewek ui, pinter, volunteer luar negeri, cantik lagi. sempurna banget gak tu idup, lupa nanya linkedinnya ðÿ¥², jaringan keputus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menormalisasi kata"
      ],
      "metadata": {
        "id": "fUrjL2RVw-1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TAHAP 4\n",
        "#Normalisasi setiap kata\n",
        "url1 = 'https://github.com/nasalsabila/kamus-alay/raw/master/colloquial-indonesian-lexicon.csv'\n",
        "kamus1 = pd.read_csv(url1).iloc[:,:2]\n",
        "kamus1 = kamus1.rename(columns={\"slang\": \"non_standar\",\"formal\" : \"standar\"})\n",
        "\n",
        "#Normalisasi tahap 1\n",
        "def normalize_1(text):\n",
        "    nonstdword = kamus1['non_standar'].values.tolist()\n",
        "    stdword = kamus1['standar'].values.tolist()\n",
        "    text = text.split(\" \")\n",
        "    for i in range(len(text)):\n",
        "      if text[i] in nonstdword:\n",
        "        index = nonstdword.index(text[i])\n",
        "        text[i] = stdword[index]\n",
        "    return ' '.join(map(str, text))\n",
        "\n",
        "data['Konten_Parsed_4'] = data['Konten_Parsed_3']\n",
        "data['Konten_Parsed_4'] = data['Konten_Parsed_4'].map(lambda com : normalize_1(com))\n",
        "\n",
        "# Menghilangkan angka\n",
        "data['Konten_Parsed_4'] = data['Konten_Parsed_4'].str.replace('[^a-zA-Z]+',' ')\n",
        "\n",
        "# Menghapus Multiple Whitespace\n",
        "def remove_multiple_whitespace(text):\n",
        "  text = re.sub('\\s+',' ',text)\n",
        "  return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "data['Konten_Parsed_4'] = data['Konten_Parsed_4'].apply(remove_multiple_whitespace)\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data[\"Konten\"][2])\n",
        "print(\"Hasil setelah tahap 4 :\\n\",data[\"Konten_Parsed_4\"][2])"
      ],
      "metadata": {
        "id": "Q38UPT76QRr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menghapus kata tidak bermakna"
      ],
      "metadata": {
        "id": "7ZgtK7MtxG3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TAHAP 5\n",
        "# Menghilangkan kata tidak bermakna (stopwords)\n",
        "import base64\n",
        "import requests\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords_sastrawi = factory.get_stop_words()\n",
        "\n",
        "master = \"https://github.com/masdevid/ID-Stopwords/raw/master/id.stopwords.02.01.2016.txt\"\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "\n",
        "meaningless = req.split(\"\\n\")\n",
        "stopwords_nltk = list(stopwords.words('indonesian')) + list(stopwords.words('english'))\n",
        "more_stopword = ['gue',\"beliau\",\"doang\",\"ha\",\"guia\",\"engkau\",\"ku\",\"sih\",\"nya\",\n",
        "                 \"kayak\",\"tau\",\"pikir\",\"bikin\",\"kali\",\"gara\",\"nama\",\n",
        "                 \"bilang\",\"tinggal\",\"tanyakanrl\",\"deh\",\"iya\",\"biar\",\"pa\"]\n",
        "meaningless = meaningless + stopwords_nltk + stopwords_sastrawi+more_stopword\n",
        "\n",
        "def delete_meaningless(text) :\n",
        "  result = []\n",
        "  text = text.split(\" \")\n",
        "  for i in range(len(text)):\n",
        "    if text[i] not in meaningless:\n",
        "      result.append(text[i])\n",
        "  return ' '.join(map(str,result))\n",
        "data['Konten_Parsed_5'] = data['Konten_Parsed_4'].map(lambda com : delete_meaningless(com))\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data[\"Konten\"][2])\n",
        "print(\"Hasil setelah tahap 5 :\\n\",data[\"Konten_Parsed_5\"][2])"
      ],
      "metadata": {
        "id": "C781TALwRfRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menghilangkan Imbuhan (stemming)\n",
        "Karena data terlalu besar nantinya akan membuat proses stemming menjadi sangat lama, sehingga kami membaginya menjadi 4 bagian dan melakukan stemming secara terpisah. Setelah mendapatkan hasil stemming, data disatukan kembali seperti pada sintaks berikut ini."
      ],
      "metadata": {
        "id": "Mr4PQsITyCC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TAHAP 6 Lemmatizating dan Stemming\n",
        "# Menghilangkan imbuhan (stemming)\n",
        "wn= nltk.WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "  result = []\n",
        "  text = text.split(\" \")\n",
        "  for word in text :\n",
        "    result.append(wn.lemmatize(word))\n",
        "  return ' '.join(map(str,result))\n",
        "\n",
        "# Buat kolom tambahan untuk data description yang telah dilemmatization\n",
        "data['Konten_Parsed_6'] = data['Konten_Parsed_5'].apply(lemmatization)\n",
        "\n",
        "index_to_drop = []\n",
        "for i in range(data.shape[0]) :\n",
        "  text = data[\"Konten_Parsed_6\"][i]\n",
        "  text = text.split(\" \")\n",
        "  if len(text)==1 :\n",
        "    index_to_drop.append(i)\n",
        "data = data.drop(index_to_drop).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "b0cqq_yLRpP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import clear_output\n",
        "\n",
        "# url = 'https://github.com/alzimna/Pemodelan_UAS/raw/main/data_sebelum_stem.xlsx'\n",
        "# a = 4500\n",
        "# b = 6208\n",
        "# bagianku = pd.read_excel(url)[a:b]\n",
        "# factory = StemmerFactory()\n",
        "# stemmer = factory.create_stemmer()\n",
        "# def stemming(text):\n",
        "#   result = []\n",
        "#   text = text.split(\" \")\n",
        "#   for word in text :\n",
        "#     stem_text = stemmer.stem(word)\n",
        "#     result.append(stem_text)\n",
        "#   result = ' '.join(map(str,result))\n",
        "#   return result\n",
        "\n",
        "# stem = []\n",
        "# n = bagianku.shape[0]\n",
        "# for i in range(n) :\n",
        "#   stem.append(stemming(bagianku.iloc[i][\"Konten_Parsed_6\"]))\n",
        "#   print(i,\"/\",n)\n",
        "#   clear_output(wait=True)\n",
        "\n",
        "# # Buat kolom tambahan untuk data description yang telah dilemmatization\n",
        "# bagianku['Konten_Parsed_6'] = stem\n",
        "\n",
        "# bagianku.to_excel(\"bagianku_setelah_stem.xlsx\",index = None)"
      ],
      "metadata": {
        "id": "nUC4zqiMUufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = [\"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_rara.xlsx\",\n",
        "       \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_maya.xlsx\",\n",
        "       \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_raisa.xlsx\",\n",
        "       \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_alzim.xlsx\"\n",
        "\n",
        "]\n",
        "data = pd.DataFrame()\n",
        "for i in range(4) :\n",
        "  temp = pd.read_excel(url[i])\n",
        "  data = pd.concat([data,temp],axis = 0)\n",
        "data.reset_index(inplace = True,drop = True)\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data.iloc[2][\"Konten\"])\n",
        "print(\"Hasil setelah tahap 6 :\\n\",data.iloc[2][\"Konten_Parsed_6\"])"
      ],
      "metadata": {
        "id": "JiwKkHjoX7b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Menghapus stopword setelah distemming\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords_sastrawi = factory.get_stop_words()\n",
        "\n",
        "master = \"https://github.com/masdevid/ID-Stopwords/raw/master/id.stopwords.02.01.2016.txt\"\n",
        "req = requests.get(master)\n",
        "req = req.text\n",
        "\n",
        "meaningless = req.split(\"\\n\")\n",
        "stopwords_nltk = list(stopwords.words('indonesian')) + list(stopwords.words('english'))\n",
        "more_stopword = ['gue',\"beliau\",\"doang\",\"ha\",\"guia\",\"engkau\",\"ku\",\"sih\",\"nya\",\n",
        "                 \"kayak\",\"tau\",\"pikir\",\"bikin\",\"kali\",\"gara\",\"nama\",\n",
        "                 \"bilang\",\"tinggal\",\"tanyakanrl\",\"deh\",\"iya\",\"biar\",\"pa\",\n",
        "                 \"cv\",\"p\",\"wa\",\"jasa\",\"jokitugas\",\"cover\",\"itv\",\"hm\",\"jasacvats\",\"kak\",\"tanyarlfes\",\n",
        "                 \"surat\",\"lamar\",\"republikaonline\",\"cek\",\"letter\",\"warpin\",\"convomf\"]\n",
        "meaningless = meaningless + stopwords_nltk + stopwords_sastrawi+more_stopword\n",
        "\n",
        "data['Konten_Parsed_6'] = data['Konten_Parsed_6'].apply(delete_meaningless)"
      ],
      "metadata": {
        "id": "FDbikaMwlx7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Data"
      ],
      "metadata": {
        "id": "lN84h-zp1Ijm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "def tokenize(text):\n",
        "  token = nltk.word_tokenize(text)\n",
        "  return token\n",
        "data[\"tokenize\"] = data[\"Konten_Parsed_6\"].apply(tokenize)"
      ],
      "metadata": {
        "id": "FnyGMvw80ntg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixing Data\n",
        "data = data.drop(['Konten_Parsed_1','Konten_Parsed_2','Konten_Parsed_3','Konten_Parsed_4',\n",
        "           'Konten_Parsed_5'],axis = 1)\n",
        "data = data.rename(columns={'Konten_Parsed_6' : 'Konten_Parsed'})\n",
        "\n",
        "print(\"Sampel data sebelum preprocess :\\n\",data.iloc[2][\"Konten\"])\n",
        "print(\"Hasil setelah tahap 6 :\\n\",data.iloc[2][\"Konten_Parsed\"])"
      ],
      "metadata": {
        "id": "Niar0L-90taz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(20)"
      ],
      "metadata": {
        "id": "Z_VbEnPm1mZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANALISIS DATA EKSPLORATIF\n",
        "data_eda = data.copy()\n",
        "data_eda.info()\n",
        "data_eda = pd.DataFrame(data_eda)"
      ],
      "metadata": {
        "id": "FGWnEdDZm_hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat grafik untuk 10 kata teratas yang paling banyak muncul di seluruh korpus\n",
        "# Gabungkan teks yang telah diproses sebelumnya dari semua isi berita\n",
        "text = ' '.join(data_eda['Konten_Parsed'])\n",
        "\n",
        "# Tokenisasi\n",
        "words = text.split()\n",
        "\n",
        "# Hitung frekuensi setiap kata\n",
        "word_freq = pd.Series(words).value_counts()\n",
        "\n",
        "# Pilih 10 kata teratas yang paling sering\n",
        "top_words = word_freq.head(50)\n",
        "\n",
        "#print\n",
        "print(top_words)"
      ],
      "metadata": {
        "id": "FCp_GRQanJ4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(palette=\"muted\")\n",
        "fig = plt.figure(figsize = (10,6))\n",
        "ax = sns.barplot(x = top_words.index[:10], y = top_words.values[:10])\n",
        "# Adding labels for every bar\n",
        "for bar, label in zip(ax.patches, top_words.values):\n",
        "    ax.annotate(label, (bar.get_x() + bar.get_width() / 2, bar.get_height()), ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "USJUj5iPnPKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perbandingan jumlah kata sebelum dan sesudah text preprocessing\n",
        "text1 = ' '.join(data['Konten'])\n",
        "print('Total kata sebelum pre-processing adalah sebanyak {} kata'.format(len(text1)))\n",
        "text2 = ' '.join(data['Konten_Parsed'])\n",
        "print('Total kata setelah pre-processing adalah sebanyak {} kata'.format(len(text2)))"
      ],
      "metadata": {
        "id": "1VGyTaQSr2kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD CLOUDS\n",
        "text = ' '.join(data['Konten_Parsed'])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('foo.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IEzHqR1vnT7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data UNVR dan KHC"
      ],
      "metadata": {
        "id": "QJ56x6sxy4-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the start and end dates\n",
        "start_date = datetime(2023, 8, 7)\n",
        "end_date = datetime(2023, 12, 7)\n",
        "\n",
        "# Define the interval between datetime objects\n",
        "interval = timedelta(days=1)\n",
        "\n",
        "# Create a list of datetime objects within the specified range\n",
        "tanggal = [start_date + i * interval for i in range((end_date - start_date).days + 1)]"
      ],
      "metadata": {
        "id": "0ut8PvS-Y0LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/Saham%20KHC.xlsx\"\n",
        "khc = pd.read_excel(url)\n",
        "khc_full = {}\n",
        "for hari in tanggal :\n",
        "  if hari in list(khc[\"Date\"]) :\n",
        "    khc_full[hari] = float(khc[khc[\"Date\"]==hari][\"Adj Close\"])\n",
        "  else :\n",
        "    khc_full[hari] = 0\n",
        "for hari in khc_full.keys() :\n",
        "  if khc_full[hari] == 0 :\n",
        "    if khc_full[hari+interval] != 0:\n",
        "      khc_full[hari] = 1/2*(khc_full[hari-interval]+khc_full[hari+interval])\n",
        "    else :\n",
        "      khc_full[hari] = 1/3*(khc_full[hari-interval]+2*khc_full[hari+2*interval])\n",
        "khc = pd.DataFrame({\n",
        "    \"Date\" : khc_full.keys(),\n",
        "    \"Adj\" : khc_full.values()\n",
        "})\n",
        "khc"
      ],
      "metadata": {
        "id": "eLDVwHqBtDwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Label = []\n",
        "n = khc.shape[0]\n",
        "for i in range(1,n) :\n",
        "  if khc.iloc[i,1] >= khc.iloc[i-1,1] :\n",
        "    Label.append(\"Naik\")\n",
        "  else :\n",
        "    Label.append(\"Turun\")\n",
        "khc.drop(0,inplace = True)\n",
        "khc[\"Label\"] = Label"
      ],
      "metadata": {
        "id": "aYSja1nj0OdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bound = datetime(2023, 10, 7)\n",
        "n = khc.shape[0]\n",
        "\n",
        "n_b = 0\n",
        "n_a = 0\n",
        "t_b = 0\n",
        "t_a = 0\n",
        "for i in range(n) :\n",
        "  if khc.iloc[i,0] < bound :\n",
        "    if khc.iloc[i,2] == \"Naik\" :\n",
        "      n_b += 1\n",
        "    else :\n",
        "      t_b +=1\n",
        "  else :\n",
        "    if khc.iloc[i,2] == \"Naik\" :\n",
        "      n_a += 1\n",
        "    else :\n",
        "      t_a +=1\n",
        "df = pd.DataFrame({\n",
        "    \"Naik\" : [n_b,n_a],\n",
        "    \"Turun\" : [t_b,t_a]\n",
        "}, index = [\"Sebelum boikot\",\"Sesudah boikot\"])\n",
        "df"
      ],
      "metadata": {
        "id": "U7rbl-5o2f7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/UNVR.JK.xlsx\"\n",
        "unvr = pd.read_excel(url)\n",
        "unvr_full = {}\n",
        "for hari in tanggal :\n",
        "  if hari in list(unvr[\"Date\"]) :\n",
        "    unvr_full[hari] = float(unvr[unvr[\"Date\"]==hari][\"Adj Close\"])\n",
        "  else :\n",
        "    unvr_full[hari] = 0\n",
        "for hari in unvr_full.keys() :\n",
        "  if unvr_full[hari] == 0 :\n",
        "    if unvr_full[hari+interval] != 0:\n",
        "      unvr_full[hari] = 1/2*(unvr_full[hari-interval]+unvr_full[hari+interval])\n",
        "    else :\n",
        "      unvr_full[hari] = 1/3*(unvr_full[hari-interval]+2*unvr_full[hari+2*interval])\n",
        "unvr = pd.DataFrame({\n",
        "    \"Date\" : unvr_full.keys(),\n",
        "    \"Adj\" : unvr_full.values()\n",
        "})\n",
        "unvr"
      ],
      "metadata": {
        "id": "wlAZt1hqtNgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Label = []\n",
        "n = unvr.shape[0]\n",
        "for i in range(1,n) :\n",
        "  if unvr.iloc[i,1] >= unvr.iloc[i-1,1] :\n",
        "    Label.append(\"Naik\")\n",
        "  else :\n",
        "    Label.append(\"Turun\")\n",
        "unvr.drop(0,inplace = True)\n",
        "unvr[\"Label\"] = Label"
      ],
      "metadata": {
        "id": "svNEdKl0zCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unvr"
      ],
      "metadata": {
        "id": "X15YctSfIthX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bound = datetime(2023, 10, 7)\n",
        "n = unvr.shape[0]\n",
        "\n",
        "n_b = 0\n",
        "n_a = 0\n",
        "t_b = 0\n",
        "t_a = 0\n",
        "for i in range(n) :\n",
        "  if unvr.iloc[i,0] < bound :\n",
        "    if unvr.iloc[i,2] == \"Naik\" :\n",
        "      n_b += 1\n",
        "    else :\n",
        "      t_b +=1\n",
        "  else :\n",
        "    if unvr.iloc[i,2] == \"Naik\" :\n",
        "      n_a += 1\n",
        "    else :\n",
        "      t_a +=1\n",
        "df = pd.DataFrame({\n",
        "    \"Naik\" : [n_b,n_a],\n",
        "    \"Turun\" : [t_b,t_a]\n",
        "}, index = [\"Sebelum boikot\",\"Sesudah boikot\"])\n",
        "df"
      ],
      "metadata": {
        "id": "zqdkhokbz8Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Sentimen Setiap Tweet\n",
        "Perhatikan data setelah setemming berikut ini"
      ],
      "metadata": {
        "id": "nvpZ-rkx-LzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "id": "9Lyr6xg4-I_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setiap tweet mempunyai tiga jenis tendensi, yaitu positif (nilai polarity positif), netral (nilai polarity 0), dan negatif (nilai polarity negatif). Langkah selanjutnya adalah mencari nilai polarity dari setiap tweet menggunakan package TextBlob. Untuk menggunakan TextBlob, tweet berbahasa Indonesia diterjemahkan kedalam bahasa inggris terlebih dahulu. Karena data terlalu besar, kami membaginya menjadi 4 bagian. Berikut ini diberikan contoh untuk mendapatkan nilai polarity setiap tweet."
      ],
      "metadata": {
        "id": "eEFpk4Wr-RbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bagianku = data[:100]\n",
        "bagianku.reset_index(drop = True,inplace = True)\n",
        "polarity = []\n",
        "n = 100\n",
        "cek = bagianku.iloc[1][\"Konten_Parsed\"]\n",
        "translation = await translator.translate(cek,src = \"id\",dest='en')\n",
        "text_indo = TextBlob(translation.text)\n",
        "text_indo.sentiment\n",
        "for i in range(n) :\n",
        "  cek = bagianku.iloc[i][\"Konten_Parsed\"]\n",
        "  translation = await translator.translate(cek,src = \"id\",dest='en')\n",
        "  text_indo = TextBlob(translation.text)\n",
        "  analysis = text_indo.sentiment\n",
        "  polarity.append(analysis.polarity)\n",
        "  print(i,\"/\",n)\n",
        "  clear_output(wait=True)\n",
        "bagianku[\"polarity\"] = polarity\n",
        "bagianku.head(10)"
      ],
      "metadata": {
        "id": "hlrVgeThAGKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = [\n",
        "    \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_rara_polarity.xlsx\",\n",
        "    \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_maya_polarity.xlsx\",\n",
        "    \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_raisa_polarity.xlsx\",\n",
        "    \"https://github.com/alzimna/Portfolio/raw/refs/heads/main/Stock%20Price%20Prediction%20with%20Sentiment%20Analysis/data/bagian_alzim_polarity.xlsx\"\n",
        "\n",
        "]\n",
        "data_polarity = pd.DataFrame()\n",
        "for i in range(len(url)) :\n",
        "  temp = pd.read_excel(url[i])\n",
        "  data_polarity = pd.concat([data_polarity,temp],axis = 0)\n",
        "data_polarity.reset_index(inplace = True,drop = True)\n",
        "data_polarity"
      ],
      "metadata": {
        "id": "RPz2sPx5AZaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(polarity):\n",
        "    if polarity < 0:\n",
        "        return 'Negatif'\n",
        "    elif polarity == 0:\n",
        "        return 'Netral'\n",
        "    else:\n",
        "        return 'Positif'\n",
        "\n",
        "# Add a new column 'Sentimen' to the DataFrame based on sentiment analysis\n",
        "data_polarity['Sentimen'] = data_polarity['polarity'].apply(analyze_sentiment)\n",
        "\n",
        "# Count the number of positive, negative, and neutral sentiments\n",
        "sentiment_counts = data_polarity['Sentimen'].value_counts()\n",
        "\n",
        "# Calculate the percentages\n",
        "total_entries = len(data_polarity)\n",
        "percentage_positif = (sentiment_counts.get('Positif', 0) / total_entries) * 100\n",
        "percentage_negatif = (sentiment_counts.get('Negatif', 0) / total_entries) * 100\n",
        "percentage_netral = (sentiment_counts.get('Netral', 0) / total_entries) * 100\n",
        "\n",
        "print(\"Persentase sentimen:\")\n",
        "print(f\"Positif: {percentage_positif:.2f}%\")\n",
        "print(f\"Negatif: {percentage_negatif:.2f}%\")\n",
        "print(f\"Netral: {percentage_netral:.2f}%\")"
      ],
      "metadata": {
        "id": "oiHqLlufRTZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah mendapatkan nilai polarity setiap tweet, dihitung rata-rata polarity dari tweet pada hari yang sama. Kemudian disatukan dengan data harga saham dan label kenaikannya."
      ],
      "metadata": {
        "id": "N9-2cMWtFI8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = data_polarity.groupby(\"Tanggal\")[\"polarity\"].mean().reset_index()\n",
        "unvr = unvr[unvr[\"Date\"] >= datetime(2023, 10, 7)].reset_index(drop = True)\n",
        "unvr = pd.concat([unvr,p.iloc[:,1]],axis = 1)\n",
        "unvr"
      ],
      "metadata": {
        "id": "NlDXbT5vLMkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unvrp = unvr.drop(['Adj','Label'], axis = 1)\n",
        "unvrp = pd.DataFrame(unvrp)\n",
        "unvrp.head()"
      ],
      "metadata": {
        "id": "x6CIlXHdHC6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat kolom label\n",
        "unvr['Label'] = unvr['Adj'].shift(-1)  # Shift harga satu baris ke atas\n",
        "unvr['Label'] = unvr.apply(lambda row: 'Naik' if row['Label'] > row['Adj'] else 'Turun', axis=1).shift(+1)\n",
        "unvr = unvr.drop(0).reset_index(drop = True)\n",
        "unvr"
      ],
      "metadata": {
        "id": "Z_9dQ09RSjGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Initialize StandardScaler\n",
        "scaler = MinMaxScaler()\n",
        "unvr[\"Adj\"] = scaler.fit_transform(unvr[[\"Adj\"]])\n",
        "unvr.head()"
      ],
      "metadata": {
        "id": "Lm6_9uh7T8sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "unvr.plot(x='Date', y=['Adj', 'polarity'], kind='line', marker='o')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BFCokOvBSmL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_tanpa_polarity = unvr.iloc[3:,].copy().drop([\"Adj\",\"Date\",\"polarity\"],axis = 1).reset_index(drop = True)\n",
        "n = class_tanpa_polarity.shape[0]\n",
        "adj_d1 = []\n",
        "adj_d2 = []\n",
        "adj_d3 = []\n",
        "for i in range(n) :\n",
        "  adj_d1.append(unvr.iloc[i+2,1])\n",
        "\n",
        "for i in range(n) :\n",
        "  adj_d2.append(unvr.iloc[i+1,1])\n",
        "\n",
        "for i in range(n) :\n",
        "  adj_d3.append(unvr.iloc[i,1])\n",
        "class_tanpa_polarity[\"adj_d1\"] = adj_d1\n",
        "class_tanpa_polarity[\"adj_d2\"] = adj_d2\n",
        "class_tanpa_polarity[\"adj_d3\"] = adj_d3\n",
        "class_tanpa_polarity"
      ],
      "metadata": {
        "id": "ZJZDWxXYWa2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_polarity = class_tanpa_polarity.copy()\n",
        "n = class_polarity.shape[0]\n",
        "polarity_d1 = []\n",
        "polarity_d2 = []\n",
        "polarity_d3 = []\n",
        "for i in range(n) :\n",
        "  polarity_d1.append(unvr.iloc[i+2,3])\n",
        "\n",
        "for i in range(n) :\n",
        "  polarity_d2.append(unvr.iloc[i+1,3])\n",
        "\n",
        "for i in range(n) :\n",
        "  polarity_d3.append(unvr.iloc[i,3])\n",
        "class_polarity[\"polarity_d1\"] = polarity_d1\n",
        "class_polarity[\"polarity_d2\"] = polarity_d2\n",
        "class_polarity[\"polarity_d3\"] = polarity_d3\n",
        "class_polarity"
      ],
      "metadata": {
        "id": "_UqAXNnsfKLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install xlsxwriter\n",
        "# with pd.ExcelWriter(\"data_klasifikasi.xlsx\", engine='xlsxwriter') as writer:\n",
        "#     class_tanpa_polarity.to_excel(writer, sheet_name='class_tanpa_polarity', index=False)\n",
        "#     class_polarity.to_excel(writer, sheet_name='class_polarity', index=False)"
      ],
      "metadata": {
        "id": "HIsMOJ_Yf81q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regresi Logistik**"
      ],
      "metadata": {
        "id": "uVjqtlMnje8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanpa Polarity"
      ],
      "metadata": {
        "id": "wQzrUU6nTs4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "swxaCALvjif4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "data = class_tanpa_polarity\n",
        "\n",
        "#Membuat Variabel X dan Y\n",
        "X = data[data.columns[1:4]]\n",
        "Y = data['Label']"
      ],
      "metadata": {
        "id": "0mca3CDMkyHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import logistic regression classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# defining parameter range\n",
        "param_grid = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C' : np.logspace(-4, 4, 20),\n",
        "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
        "    'max_iter' : [100, 1000,2500, 5000]\n",
        "    }\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3)\n",
        "\n",
        "# fitting the model for grid search\n",
        "grid.fit(X, Y)\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "jiBhjlcsSGC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid.predict(X)\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "cm_LR=confusion_matrix(Y, y_pred)\n",
        "print(cm_LR)\n",
        "# from sklearn.metrics import classification_report\n",
        "print(classification_report(Y, y_pred))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm_LR, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "5p_APvogoN8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LR dengan Polarity"
      ],
      "metadata": {
        "id": "rxLiVBcuTmXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "data = class_polarity\n",
        "\n",
        "#Membuat Variabel X dan Y\n",
        "X_p = data[data.columns[1:]]\n",
        "Y_p = data['Label']"
      ],
      "metadata": {
        "id": "baqS5XVpUAzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import logistic regression classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# defining parameter range\n",
        "param_grid = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C' : np.logspace(-4, 4, 20),\n",
        "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
        "    'max_iter' : [100, 1000,2500, 5000]\n",
        "    }\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3)\n",
        "\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_p, Y_p)\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "HqB5rqiHUAzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_p = grid.predict(X_p)\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "cm_LR=confusion_matrix(Y_p, y_pred_p)\n",
        "print(cm_LR)\n",
        "# from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_p, y_pred_p))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm_LR, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "FPnL_3bPUAzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Discriminant Analysis (LDA)"
      ],
      "metadata": {
        "id": "OYwGCkaDU-lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pingouin\n",
        "import pingouin as pg\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Z-DVrivsiUkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = class_tanpa_polarity\n",
        "pg.box_m(data, dvs=list(data.columns)[1:], group='Label')"
      ],
      "metadata": {
        "id": "IAjHrtc1icjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanpa Polarity"
      ],
      "metadata": {
        "id": "eCkBCsFXYYAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "data = class_tanpa_polarity\n",
        "\n",
        "#Membuat Variabel X dan Y\n",
        "X_LDA = data[data.columns[1:4]]\n",
        "Y_LDA = data['Label']\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "model = lda.fit(X_LDA,Y_LDA)\n",
        "pred = model.predict(X_LDA)\n",
        "\n",
        "#print(confusion_matrix(Y_LDA, pred))\n",
        "print(classification_report(Y_LDA, pred))"
      ],
      "metadata": {
        "id": "Kqu38w3rVN9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cm_LDA=confusion_matrix(Y_LDA, pred)\n",
        "sns.heatmap(cm_LDA, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "sgIdyZpuZcoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA dengan Polarity"
      ],
      "metadata": {
        "id": "ANIihVF0YaOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "data = class_polarity\n",
        "\n",
        "#Membuat Variabel X dan Y\n",
        "X_LDA_p = data[data.columns[1:]]\n",
        "Y_LDA_p = data['Label']\n",
        "\n",
        "lda =  LinearDiscriminantAnalysis()\n",
        "model = lda.fit(X_LDA_p,Y_LDA_p)\n",
        "pred_p = model.predict(X_LDA_p)\n",
        "\n",
        "print(confusion_matrix(Y_LDA_p, pred_p))\n",
        "print(classification_report(Y_LDA_p, pred_p))"
      ],
      "metadata": {
        "id": "rVjfMQg1YWfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cm_LDA=confusion_matrix(Y_LDA_p, pred_p)\n",
        "sns.heatmap(cm_LDA, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "eBvRTtixZ1F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neigbour"
      ],
      "metadata": {
        "id": "XUHOc8iA3A-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tanpa Polarity"
      ],
      "metadata": {
        "id": "IKodrWDn_ngx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "hxWU1uHs3Kj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = class_tanpa_polarity\n",
        "\n",
        "#menentukan variabel independen\n",
        "x = data.drop(['Label'],axis = 1)\n",
        "\n",
        "#menentukan variabel dependen\n",
        "y = data['Label']\n",
        "\n",
        "n_terbaik = 0\n",
        "maks_akurasi = 0\n",
        "\n",
        "#menentukan n terbaik KNN\n",
        "\n",
        "for i in range(2,10):\n",
        "  knn = KNeighborsClassifier(n_neighbors = i)\n",
        "  knn.fit(x,y)\n",
        "\n",
        "  y_pred = knn.predict(x)\n",
        "\n",
        "  akurasi = accuracy_score(y, y_pred)\n",
        "\n",
        "  print(f'Akurasi for n={i}: {akurasi}')\n",
        "  if akurasi > maks_akurasi:\n",
        "        maks_akurasi = akurasi\n",
        "        n_terbaik = i\n",
        "\n",
        "# Cetak nilai n_terbaik dan maks_akurasi setelah loop selesai\n",
        "print(f'Nilai n terbaik: {n_terbaik}, Maksimum Akurasi: {maks_akurasi:.4f}')"
      ],
      "metadata": {
        "id": "myF_gJKagiSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN untuk n terbaik\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(x,y)\n",
        "\n",
        "y_pred = knn.predict(x)\n",
        "\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "print('confussion matrix:')\n",
        "print(confusion_matrix(y, y_pred))\n",
        "\n",
        "print('classification report:')\n",
        "print(classification_report(y, y_pred))\n",
        "\n",
        "print('nilai akurasi:')\n",
        "print(accuracy_score(y, y_pred))"
      ],
      "metadata": {
        "id": "sxtE7Y_w3StL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "8tLz5Nc853I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN dengan Polarity"
      ],
      "metadata": {
        "id": "PnaJWRve_vCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = class_polarity\n",
        "\n",
        "#menentukan variabel independen\n",
        "x = data.drop(['Label'],axis = 1)\n",
        "\n",
        "#menentukan variabel dependen\n",
        "y = data['Label']\n",
        "\n",
        "n_terbaik = 0\n",
        "maks_akurasi = 0\n",
        "\n",
        "#menentukan n terbaik KNN\n",
        "\n",
        "for i in range(2,10):\n",
        "  knn = KNeighborsClassifier(n_neighbors = i)\n",
        "  knn.fit(x,y)\n",
        "\n",
        "  y_pred = knn.predict(x)\n",
        "\n",
        "  akurasi = accuracy_score(y, y_pred)\n",
        "\n",
        "  print(f'Akurasi for n={i}: {akurasi}')\n",
        "  if akurasi > maks_akurasi:\n",
        "        maks_akurasi = akurasi\n",
        "        n_terbaik = i\n",
        "\n",
        "# Cetak nilai n_terbaik dan maks_akurasi setelah loop selesai\n",
        "print(f'Nilai n terbaik: {n_terbaik}, Maksimum Akurasi: {maks_akurasi:.4f}')"
      ],
      "metadata": {
        "id": "jLKE-FQw_yq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN untuk n terbaik\n",
        "knn = KNeighborsClassifier(n_neighbors = 2)\n",
        "knn.fit(x,y)\n",
        "\n",
        "y_pred = knn.predict(x)\n",
        "\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "print('confussion matrix:')\n",
        "print(confusion_matrix(y, y_pred))\n",
        "\n",
        "print('classification report:')\n",
        "print(classification_report(y, y_pred))\n",
        "\n",
        "print('nilai akurasi:')\n",
        "print(accuracy_score(y, y_pred))"
      ],
      "metadata": {
        "id": "UkN4s7W1_5Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "otqd3huW_9n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "hqXr657JCQRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM (Tanpa Polaritas)"
      ],
      "metadata": {
        "id": "YfGHggCS8ffO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = class_tanpa_polarity\n",
        "\n",
        "#Membuat Variabel X dan Y\n",
        "X_SVM = data[data.columns[1:4]]\n",
        "Y_SVM = data['Label']"
      ],
      "metadata": {
        "id": "q8tMmlJ08hGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# defining parameter range\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
        "\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_SVM, Y_SVM)\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "ljAqqxi_PHjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_SVM = grid.predict(X_SVM)\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "cm_SVM=confusion_matrix(Y_SVM, y_pred_SVM)\n",
        "print(cm_SVM)\n",
        "# from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_SVM, y_pred_SVM))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm_SVM, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "UgjkQPXrPiB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM (Dengan Polarity)"
      ],
      "metadata": {
        "id": "cQJXa6l8-27X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "datap = class_polarity\n",
        "\n",
        "#Membuat Variabel X dan Y\n",
        "X_SVM_p = datap[datap.columns[1:7]]\n",
        "Y_SVM_p = datap['Label']"
      ],
      "metadata": {
        "id": "tBSEZXQB-5fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# defining parameter range\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
        "\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_SVM_p, Y_SVM_p)\n",
        "\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)"
      ],
      "metadata": {
        "id": "Zly-zU0BQFMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_SVM_p = grid.predict(X_SVM_p)\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "cm_SVM_p=confusion_matrix(Y_SVM_p, y_pred_SVM_p)\n",
        "print(cm_SVM_p)\n",
        "# from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_SVM_p, y_pred_SVM_p))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm_SVM_p, annot=True,fmt=\"\",cmap='RdYlGn',linewidths=0.30)"
      ],
      "metadata": {
        "id": "eacpclXvOLyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa dari keempat model klasifikasi, **Polarity akan menaikkan akurasi model jika dibandingkan dengan data tanpa polaritas. Artinya analisis sentimen dapat membantu untuk menaikkan akurasi model biasa untuk mengklasifikasian Naik/Turunnya harga saham**."
      ],
      "metadata": {
        "id": "pdXThHz8KO0y"
      }
    }
  ]
}